{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d8728f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Different machine learners\n",
    "* gives an overview on popular machine learners in a nutshell.\n",
    "* Lots of site on the internet give great detail on this and you should take a few minuted to check them out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc854e57",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In general, all these approaches work by extracting *features* from data and \n",
    "* comparing a *test* sample's features with the features derived from a *training* set \n",
    "* to predict some *class*  or *value*  [in case of regression](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Classification_vs_Regression).\n",
    "\n",
    "So they work with two phases: \n",
    "\n",
    "* During training, the *parameters* of the approach are learned, thereby creating the *model*.\n",
    "* A test time, unknown test samples get *predicted* by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d91fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition, most of these approaches can be customized by \n",
    "* *meta-parameters* which also can be learned by some \n",
    "* *meta algorithm*, but not during a normal training. \n",
    "\n",
    "One thing all of these approaches have in common is that they \n",
    "* *model* the world by \"densing\" down  the real values, \n",
    "* i.e. the data, to a simpler form at some time (*feature extraction*), \n",
    "\n",
    "so they all can be seen as some kind of dimensionality reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3126b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* On the one hand you lose information this way, \n",
    "* on the other this is not a problem because you usually are interested in some kind of underlying principle that generated your training data, \n",
    "* and not so much in the training data itself.\n",
    "\n",
    "Still you got a trade-off between \n",
    "* generalizability and\n",
    "* [specificity](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Bias_vs_variance)\n",
    "* aka *bias* vs. *variance*\n",
    "\n",
    "The following list is by far not complete, I simply selected the ones that were most commonly used during my professional life."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8e40e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear regression\n",
    "To represent the dependency of a dependend and an independend variable by a straight line. \n",
    "* The price question is how to learn the two parameters of  the line (*a* and *b* of *y=ax+b*) using the training data. \n",
    "* One approach would be gradient descent with a [Perceptron](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Perceptron).\n",
    "<img src=images/linear_regression.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b79de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GMMs\n",
    "\n",
    "A Gaussian is a way to describe \n",
    "* a distribution with two values: mean and variance. \n",
    "* One way to distinguish two kinds of things is two distinguish them by the distributions of their features,\n",
    "* e.g. herrings from trouts by the size of their fins.\n",
    "* Gaussian mixture models model *one* distribution of each feature by a mix of several Gaussians, hence their name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b853a9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/gmms.png width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98c97d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## (Naive) Bayes\n",
    "* Bayes statistics is fundamentally different from so-called frequentist statistics, as it takes prior knowledge of the problem into account.\n",
    "* The Bayesian formula tells us how likely an event (the class we want to distinguish) can happen in conjunction with another event (the feature that we observe).\n",
    "* During training the Bayes classifier updates its believe about the world, using absolute or estimated frequencies as prior knowledge.\n",
    "* The approach is called naive because it assumes that each input feature is independent, which is most of the time not true.\n",
    "<img src=images/naive_bayes.png width=20%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9282d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## KNN (k nearest neighbor)\n",
    "\n",
    "* K nearest neighbor is an approach to assign *test* data, \n",
    "* given its *k* (given parameter) nearest neighbors (in the *feature* space, by some distance metrics) \n",
    "* either the most common *class* or some property *value* as an average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af57950",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/knn.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c27092",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Support vector machines\n",
    "* Support vector machines are algorithms motivated by vector geometry\n",
    "* They construct *hyperplanes* in N-dimensional (number of *features*) space by maximizing the margin between data points from different classes.\n",
    "* The function that defines the hyperplane is called the kernel function and can be parameterized.\n",
    "* They can be combined with GMMS if the data is approximated by them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417dc713",
   "metadata": {},
   "source": [
    "<img src=images/svm.png width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c3197",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CART (classification and regression trees)\n",
    "* Perhaps the most straightforward way to categorize data: order its parameters in a tree like fashion with the *features* as twigs and the data points as leaves.\n",
    "* The tree is learned from the training set (and can be probabilistic).\n",
    "* The big advantage of this model is that it is easily interpretable to humans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849d1d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/cart.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99919884",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## XGBoost\n",
    "* A sophisticated algorithm loosely based on CARTS \n",
    "* as it combines Random Forests (ensembles of trees) \n",
    "* with boosting more successful ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d31120d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/xgboost.png width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450157d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MLP (Multi-layer perceptron)\n",
    "\n",
    "As the name suggests, these algorithms are derived from the original [Perceptron](http://blog.syntheticspeech.de/2022/02/16/kinds-of-machine-learning/#Perceptron) idea that is inspired by the human brain.\n",
    "<img src=images/ann.png width=40%>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
